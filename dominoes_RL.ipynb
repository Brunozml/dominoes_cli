{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81c0a736",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dominoes\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923b096",
   "metadata": {},
   "source": [
    "# Tabular Q-Learning for dominoes\n",
    "in this notebook, we'll be developing a RL agent inside of the dominoes library.\n",
    "\n",
    "\n",
    "---\n",
    "**Notes to self**\n",
    "- [ ] trouble with move proposal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119e8713",
   "metadata": {},
   "source": [
    "# 1. Observing the environment\n",
    "\n",
    "for now, we'll assume player 0 always starts. In the following block, a game is initialized and its state is displayed. We assume each player only knows her hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6de102b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [6|6]\n",
       "Player 0's hand: [2|6][5|5][0|0][1|1][0|3][0|2][0|5]\n",
       "Player 1's hand: [4|5][0|1][1|5][2|5][4|6][1|4][3|3]\n",
       "Player 2's hand: [2|4][1|3][3|5][4|4][5|6][2|2][1|2]\n",
       "Player 3's hand: [3|4][2|3][3|6][0|4][1|6][0|6]\n",
       "Player 0's turn"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#agent with be player 0\n",
    "game = dominoes.Game.new(starting_domino = dominoes.Domino(6, 6) )\n",
    "\n",
    "game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c12cd2",
   "metadata": {},
   "source": [
    "now, we display i) the state of the board and ii) the agents hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "757fef7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(, [2|2][1|2][1|1][3|5][0|5][3|6][1|5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.board, game.hands[0] # state of the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a72ed63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " turn = game.turn; turn # indicates which player is next (0-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92b4e10",
   "metadata": {},
   "source": [
    "# 2. Executing an action\n",
    "\n",
    "here we take a look at exactly how an agent can take an action that influences the board.\n",
    "\n",
    "\n",
    "\n",
    "The following method provides the a tuple of the current player's dominoes and a boolean indicating whether it can be played or not. The automated players that have already been implemented basically just reorder this tuple of tuples. \n",
    "\n",
    "---\n",
    "I have some questions here:\n",
    "- Why is this done this way? could I do it another way?\n",
    "- what exactly does the valid_moves method do? (check code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de0870ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([2|2], True),\n",
       " ([1|2], True),\n",
       " ([1|1], True),\n",
       " ([3|5], True),\n",
       " ([0|5], True),\n",
       " ([3|6], True),\n",
       " ([1|5], True))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# before applying the player [ranking] algorithm\n",
    "game.valid_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b22eb9d",
   "metadata": {},
   "source": [
    "now we take a look at how some of the automated players produce there moves. In particular, we notice that when the player is called on the game object, it restructures the current player's hand according to its preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc876952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# automated players prefers to play highest domino.\n",
    "bota_gorda = dominoes.players.bota_gorda\n",
    "\n",
    "# apply the player setting to select a\n",
    "# move to play. the player setting is a callable that will sort the\n",
    "# game's valid moves in decreasing order of preference.\n",
    "bota_gorda(game) # can see them as game.valid_moves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5777cb2",
   "metadata": {},
   "source": [
    "the new ranking of the player's hand is now different...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29ac3540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(([3|6], True),\n",
       " ([3|5], True),\n",
       " ([1|5], True),\n",
       " ([0|5], True),\n",
       " ([2|2], True),\n",
       " ([1|2], True),\n",
       " ([1|1], True))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# after applying the player [ranking] algorithm\n",
    "game.valid_moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6d15445e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([6|6], True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game.valid_moves[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e0ebb0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the selected move\n",
    "\n",
    "game.make_move(*game.valid_moves[0]) # why the asterisk??? \n",
    "# asterisk argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cfb48cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Board: [6|6]\n",
       "Player 0's hand: [3|4][4|5][4|6][4|4][2|5][0|6]\n",
       "Player 1's hand: [3|3][2|3][1|6][3|5][1|1][1|5][1|4]\n",
       "Player 2's hand: [0|5][0|0][5|5][2|6][5|6][0|3][1|2]\n",
       "Player 3's hand: [0|2][0|1][2|2][3|6][0|4][2|4][1|3]\n",
       "Player 1's turn"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec64c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acae5752",
   "metadata": {},
   "source": [
    "# 3. Building an agent\n",
    "\n",
    "- [X] determining the reward function\n",
    "- [ ] epsilon-greedy exploration algorithm\n",
    "- [ ] Q- learning\n",
    "\n",
    "## i.  Rewards\n",
    "\n",
    "Reward schedule:\n",
    "\n",
    "- Win: +points earned\n",
    "- Game continues: 0\n",
    "- Lose: -points earned\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a1927d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(game):\n",
    "    \n",
    "    # game hasn't finished\n",
    "    if game.result is not None:\n",
    "        # if agent teams wins, reward = points earned\n",
    "        if game.result.player == 0 or game.result.player == 2:\n",
    "            # return 1\n",
    "            return game.result.points\n",
    "        # if agent teams wins, reward = -( points lost)\n",
    "        else:\n",
    "            # return -1\n",
    "            return -game.result.points\n",
    "    \n",
    "    # if game hasn't finished, do not return\n",
    "    return 0\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4c05e09e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880c0eda",
   "metadata": {},
   "source": [
    "## ii. e-greedy algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "02debbb2",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3269743613.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [53], line 4\u001b[0;36m\u001b[0m\n\u001b[0;31m    def get_action(self, obs: tuple[int, int, bool]) -> int:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "### code obtained from \n",
    "### https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    " ### ---- will comment out algorithm for now\n",
    "    \n",
    "#     self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "#     def get_action(self, obs: tuple[int, int, bool]) -> int:\n",
    "        \n",
    "#         \"\"\"\n",
    "#         Returns the best action with probability (1 - epsilon)\n",
    "#         otherwise a random action with probability epsilon to ensure exploration.\n",
    "#         \"\"\"\n",
    "#         # with probability epsilon return a random action to explore the environment\n",
    "#         if np.random.random() < self.epsilon:\n",
    "#             return env.action_space.sample()\n",
    "\n",
    "#         # with probability (1 - epsilon) act greedily (exploit)\n",
    "#         else:\n",
    "#             return int(np.argmax(self.q_values[obs]))\n",
    "\n",
    "#         ...\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463e54d5",
   "metadata": {},
   "source": [
    "## iii. Q-learning\n",
    "\n",
    "- `self.q_values`: This is a dictionary that will store Q-values for each state-action pair in a reinforcement learning problem. The keys of this dictionary represent states, and the values are arrays representing the Q-values for each possible action in that state.\n",
    "\n",
    "- `defaultdict`: defaultdict is a type of dictionary provided by the collections module in Python. Here we're using defaultdict to ensure that if you access a state that hasn't been seen before, it will automatically create a new entry for that state with a default value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "177f99bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'self' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [57], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# code obtained from \u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39mq_values \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: np\u001b[38;5;241m.\u001b[39mzeros(env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
     ]
    }
   ],
   "source": [
    "# code obtained from \n",
    "# https://gymnasium.farama.org/environments/toy_text/frozen_lake/\n",
    "\n",
    "self.q_values = defaultdict(lambda: np.zeros(env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc984bb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
